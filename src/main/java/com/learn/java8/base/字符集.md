编码字符集里的每一个字符规定的顺序，叫码点( code point )，而这个字符在编码字符集里的序号，在给定的编码方式下的二进制序列叫码元( code unit )。


 Java 编码体系中要区分清楚内码和外码：

1）内码是程序内部使用的字符编码，特别是某种语言实现其 char 或 String 类型在内存里用的内部编码；

2）外码是程序与外部交互时外部使用的字符编码。“外部”相对“内部”而言；不是 char 或 String 在内存里用的内部编码的地方都可以认为是“外部”。例如，外部可以是序列化之后的 char 或 String，或者外部的文件、命令行参数之类的。

Java 语言规定：Java 的 char 类型是 UTF-16 的 code unit ，也就是一定是16位（2字节）；然后字符串是 UTF-16 code unit 的序列；这样，Java规定了字符的内码要用UTF-16编码。


在 UTF-16 设计之初，采用的是定长编码，尴尬的是，随着更多字符的引入，尤其是汉语、韩语和日文中的表意文字的引入，使得 Unicode 远远超出 16 比特编码的范围。当单元固定长度 16 位的UTF-16 到达容量上限不能支持更多的 Unicode 字符的时候，Unicode 协会放弃了定长 UTF-16，取而代之的是 16 位的变长编码 UTF-16。

这样 UTF-16 就变成了坑爹的变长编码（也就是说一个完整的“字符”是一个码点，一个码点可以对应 1 到 2 个码元，一个码元是16位）。

Java 语言深受其苦，但也只能跟进，引入码点和码元两个概念，为了实现 UTF-16 的变长编码语义，Java 规定 char 仍然只能是一个 16 位的码元，也就是说 Java 的char类型不一定能表示一个UTF-16的“字符”， 只有只需1个码元的码点才可以完整的存在 char 里。

String 作为 char 的序列，可以包含由两个码元组成的 “surrogate pair” 来表示需要 2 个码元表示的 UTF-16 码点，为此 Java 的标准库新加了一套用于访问码点的 API，而这套 API 就表现出了 UTF-16 的变长特性。


## 字节端序

字节序，又称字节顺序，其英文为Byte-Order；另外英文中还可称为Endianness，因此也翻译为端序。

格列佛游记： 小人国的国王下了一道指令，规定其人民在剥水煮蛋时必须从little-end(小端)开始剥。这个规定惹恼了一群觉得应该要从big-end(大端)开始剥的人。事情发展到后来演变成了一场战争。后来，支持小端的人被称为little-endian，反之则被称为big-endian(在英语中后缀“-ian”表示“xx人”之意)。

### 为什么会存在字节序的问题？

因为历史上设计不同计算机系统的人在当时基于各自的理由和原因(这里的理由和原因网上存在着各种不同的说法，但也或许根本就没有具体理由和原因，只是设计人员的个人偏好，甚至是随意决定的)，在各自计算机系统的设计上作出了不同的选择。

字节序共分为三种：

* 大端序BE(Big-Endian，也称高尾端序)；
* 小端序LE(Little-Endian，也称为低尾端序)；
* 中间序ME(Middle-Endian，也称为混合序)，不太常用。

字节序，具体来说，就是多字节数据(大于一个字节的数据)在计算机中存储、读取时其各个字节的排列顺序。

字节序也被称为端序，这里的“端”，是指多字节数据中位于两端的字节，很多情况下还特指尾端字节(也称为小端字节)。

所谓尾端字节或小端字节，假设按照人对文字通常从左到右(或从上到下)的读写顺序来看的话，多字节数据位于右端(或下端)的低位字节就是尾端字节或小端字节，而将位于左端(或上端)的高位字节称为头端字节或大端字节。

当然，如果不按照通常从左到右的顺序，而是按照从右到左的顺序，那么多字节数据位于右端的高位字节就是头端字节或大端字节，而将位于左端的低位字节称为尾端字节或小端字节。

可见，不论读写顺序如何，所谓大端、头端，指的是多字节数据中，代表更大数值的那个字节所在的那一端，而相反的那一端则是小端、尾端。

人读写二进制数的方向为(这是确定不变的)：左--->右，大端/头端/高位--->小端/尾端/低位；或上--->下，大端/头端/高位--->小端/尾端/低位；

内存地址的增长方向则为(这是确定不变的)：左--->右，低地址--->高地址；或上--->下，低地址--->高地址。

不过，计算机在内存中存取数据的方向则不是确定不变的，而是分为两种(注意，由于人的读写方向和内存地址增长方向是确定不变的，因此这里指的是计算机在内存中“书写”或“阅读”数据的方向)：

1） 左--->右，大端/头端/高位--->小端/尾端/低位；或上--->下，大端/头端/高位--->小端/尾端/低位；

这种情况下，站在人的读写方向和内存地址增长方向(这两者的方向刚好一致)的角度来看，则是：大端在左(或在上)，所以称之为大端序；或者说尾端在内存高地址，所以称之为高尾端序(即内存高地址存放多字节数据的尾端字节的字节顺序)。

2） 右--->左，大端/头端/高位--->小端/尾端/低位；或下--->上，大端/头端/高位--->小端/尾端/低位。

这种情况下，站在人的读写方向和内存地址增长方向(这两者的方向刚好一致)的角度来看，则是：小端在左(或在上)，所以称之为小端序；或者说尾端在内存低地址，所以称之为低尾端序(即内存低地址存放多字节数据的尾端字节的字节顺序)。

特别提示：大端序、小端序特别容易搞混，不好记忆；因此，建议使用高尾端序、低尾端序，本人是按下面这个方法来记忆的，很容易记住：存储的数据分头和尾——左头右尾，内存的地址分低和高——左低右高；因此，“高尾端”表示内存的高地址存储数据的尾端，“低尾端”表示内存的低地址存储数据的尾端。

注意，这里的“数据”指的是数据类型意义上的数据，因此，准确地说字节序只跟多字节的整型数据类型有关，比如int、short、long型；跟单字节的整型数据类型byte无关。

## 为什么就只跟多字节的整型数据有关，而跟单字节的整型数据无关呢？

因为在现代计算机中，字节是计算机数据存储的基本单位，对于整体上的单一字节(a byte)，涉及到的是其8个比特位的顺序(位序、比特序，由于一般直接由硬件处理，程序员大致了解即可，这里不深入探讨)，显然不存在字节序问题。

然而，对于在数据类型上作为一个整体的多字节数据而言，它是由各个可被单独寻址的字节所组成的(处理器寻址的最小单位一般是1个字节)，由于历史的原因，其各个字节的存储顺序在不同的系统平台(包括CPU和操作系统)上是不同的。

也就是说，如果计算机处理的数据是单字节数据类型(byte)，是不存在字节序问题的，因为单个字节已经是处理器寻址的最小单位以及存储和传输的最小单元了，存储时单字节数据类型直接进行，读取时也不存在根据前后2个字节才能解析出其值的情况，而构造字节流时也不会从一个单字节数据类型的值当中产生2个或以上的字节(既然是单字节数据类型，构造字节流时当然只可能产生1个字节)。

但是，如果计算机处理的数据是多字节数据类型(int、short、long等)，虽然由于构成它们的2个或2个以上的字节是作为一个整体来进行处理的(比如以汇编语言中的数据类型word或dword为单位进行一次性处理，而不是以byte为单位分次处理；更深入地来讲，CPU一般是以字为一个整体来处理数据的，当单个数据不足一个字长时，则将多个数据“拼成”一个字再进行处理)，但问题是字节才是CPU对内存寻址的最小单位以及存储和传输的最小单元。

因此，CPU在读取作为一个整体来进行处理的多字节数据类型的数据时，必须根据前后2个或2个以上的字节来解析出一个多字节数据类型的值；而且构造字节序列时也会从一个多字节数据类型的值当中产生2个或2个以上的字节。

这样一来，多字节数据类型的数据内部各字节间的排列顺序，是会影响从字节序列恢复到数值的；反之，也会影响从数值到字节序列的构造。

所以，在存储和读取多字节数据类型的数据时，必须按照计算机系统所规定的字节序进行(这一点程序员了解即可，计算机会自动处理)；而尤其是在跨字节序不同的异构计算机系统进行通讯并交换数据时，通讯的任何一方更是必须明确对方所采用的字节序，然后双方将各自接收到的数据按各自的字节序对数据进行转换(有时候需要程序员专门编写转换程序)，否则通讯将会出错，甚至直接失败

## 网络字节序(network byte order网络字节顺序、网络序

网络字节顺序是TCP/IP中规定好的一种数据表示格式，它与具体的CPU类型、操作系统等无关，从而可以保证数据在不同主机之间传输时能够被正确解释。IP协议中定义大端序Big Endian为网络字节序。

不过，容易令人困惑的是，IP协议作为网络层协议，其面向的数据是报文，是没有字节的概念的，也就无关字节序了。因此，英文版wikipedia上说：

In fact, the Internet Protocol defines a standard big-endian network byte order. This byte order is used for all numeric values in the packet headers and by many higher level protocols and file formats that are designed for use over IP.

也就是说，IP协议里的字节序实际上是用在分组头里的数值上的，例如每个分组头会包含源IP地址和目标IP地址，在寻址和路由的时候是需要用到这些值的。

比如，4个字节的32 bit值以下面的次序传输：首先是高位的0～7bit，其次8～15bit，然后16～23bit，最后是低位的24~31bit。这种传输次序称作大端字节序。由于TCP/IP头部中所有的二进制整数在网络中传输时都要求以这种次序，因此它又称作网络字节序。

再比如，以太网头部中2字节的“以太网帧类型”字段，表示是的后面所跟数据帧的类型。对于ARP请求或应答的以太网帧类型来说，在网络传输时，发送的顺序是以大端方式进行的：0x08，0x06。


## BOM字节序标记(Byte-Order Mark字节顺序标记)

字节序标记BOM是Unicode码点值为FEFF(十进制为65279，二进制为1111 1110 1111 1111)的字符的别名。

如果UTF-16编码的字节序列为大端序，则该字节序标记在字节流的开头呈现为0xFE 0xFF；若字节序列为小端序，则该字节序标记在字节流的开头呈现为0xFF 0xFE。如果UTF-32编码的字节序列为大端序，则该字节序标记在字节流的开头呈现为0x00 0x00 0xFE 0xFF；若字节序列为小端序，则该字节序标记在字节流的开头呈现为0xFF 0xFE 0x00 0x00。

UTF-8编码本身没有字节序的问题，但仍然有可能会用到BOM——有时被用来标示某文本是UTF-8编码格式的文本；再强调一遍：在UFT-8编码格式的文本中，如果添加了BOM，则只用它来标示该文本是由UTF-8编码方式编码的，而不用来说明字节序，因为UTF-8编码不存在字节序问题。

由于UTF-8编码方式以一个字节(8位)作为码元，属于单字节码元，在计算机处理、存储和传输时不存在字节序问题(字节序问题只跟多字节码元有关)，因此避免了平台依赖性，跨平台兼容性好。

它相对于其他编码方式对英语更为友好，同样也对计算机语言(如C++、Java、C#、JavaScript、PHP、HTML等)更为友好。它在处理ASCII等常用字符集时很少会比UTF-16低效。

UTF-8是较为平衡、较为理想的Unicode编码方式。虽然Windows平台由于历史的原因API缺乏对UTF-8的原生支持(Windows原生支持的是UTF-16，因为UTF-16早于UTF-8面世)，导致UTF-8推出后的早期使用不广，但目前是应用最为广泛的三大UTF编码方式之一。

因此，应该尽量使用UTF-8(准确地说，应该尽量使用UTF-8 without BOM，即不带字节顺序标记BOM的UTF-8)。

## UTF-8究竟是怎么编码的

UTF-8编码是Unicode字符集的一种编码方式(CEF)，其特点是使用变长字节数(即变长码元序列、变宽码元序列)来编码。一般是1到4个字节，当然，也可以更长。

为什么要变长呢？这可以理解为按需分配，比如一个字节足以容纳所有的ASCII字符，那何必补一堆0用更多的字节来存储呢？

实际上变长编码有其优势也有其劣势，优势是节省空间、自动纠错性能好、利于传输、扩展性强，劣势是不利于程序内部处理，比如正则表达式检索；而UTF-32这样等长码元序列(即等宽码元序列)的编码方式就比较适合程序处理，当然，缺点是比较耗费存储空间。

### 那UTF-8究竟是怎么编码的呢？也就是说其编码算法是什么？

UTF-8编码最短的为一个字节、最长的目前为四个字节，从首字节就可以判断一个UTF-8编码有几个字节：

* 如果首字节以0开头，肯定是单字节编码(即单个单字节码元)；
* 如果首字节以110开头，肯定是双字节编码(即由两个单字节码元所组成的双码元序列)；
* 如果首字节以1110开头，肯定是三字节编码(即由三个单字节码元所组成的三码元序列)，以此类推。
另外，UTF-8编码中，除了单字节编码外，由多个单字节码元所组成的多字节编码其首字节以外的后续字节均以10开头(以区别于单字节编码以及多字节编码的首字节)。

0、110、1110以及10相当于UTF-8编码中各个字节的前缀，因此称之为前缀码。其中，前缀码110、1110及10中的0，是前缀码中的终结标志。

UTF-8编码中的前缀码起到了很好的区分和标识的作用——当解码程序读取到一个字节的首位为0，表示这是一个单字节编码的ASCII字符；当读取到一个字节的首位为1，表示这是一个非ASCII字符的多字节编码字符中的某个字节(可能是首字节，也可能是后续字节)，接下来若继续读取到一个1，则确定为首字节，再继续读取直到遇见终结标志0为止，读取了几个1，就表示该字符为几个字节的编码；当读取到一个字节的首位为1，紧接着读取到一个终结标志0，则该字节显然是非ASCII字符的后续字节(即非首字节)。

![images/string/UTF-8.png]()

* 单字节可编码的Unicode码点值范围十六进制为0x0000 ~ 0x007F，十进制为0 ~ 127；

* 双字节可编码的Unicode码点值范围十六进制为0x0080 ~ 0x07FF，十进制为128 ~ 2047；

* 三字节可编码的Unicode码点值范围十六进制为0x0800 ~ 0xFFFF，十进制为2048 ~ 65535；

* 四字节可编码的Unicode码点值范围十六进制为0x10000 ~ 0x1FFFFF，十进制为65536 ~ 2097151（目前Unicode字符集码点编号的最大值为0x10FFFF，实际尚未编号到0x1FFFFF；这说明作为变长字节数的UTF-8编码其未来扩展性非常强，即便目前的四字节编码也还有大量编码空间未被使用，更不论还可扩展为五字节、六字节……）。

上述Unicode码点值范围中十进制值127、2047、65535、2097151这几个临界值是怎么来的呢？

因为UTF-8编码中的每个字节中都含有起到区分和标识之用的前缀码0、110、1110以及10之一，所以1～4个字节的UTF-8编码其实际有效位数分别为8-1=7位（2^7-1=127）、16-5=11位（2^11-1=2047）、24-8=16位（2^16-1=65535）、32-11=21位（2^21-1=2097151）

![images/string/UTF-8-2.png]()

上图中的Unicode range即Unicode码点值范围(也就是Unicode码点编号范围)，Hex为16进制，Binary为二进制；Encoded bytes即UTF-8编码中各字节的编码方式(即编码算法)，其中，x代表Unicode二进制码点值的单字节或低字节中的低7位或8位、y代表两字节码点值的高字节中的低3位或8位以及三字节码点值的中字节中的8位、z代表三字节码点值的高字节中的低5位。

因此，UTF-8编码的算法简单地用一句话来概括就是：首先确定UTF-8编码中各个字节的前缀码；之后再将UTF-8编码中各个字节除了前缀码所占用之外的位，依次分配给Unicode字符码点值二进制中各个位的值，换言之，就是用Unicode字符码点值二进制中各个位的值，依次填充UTF-8编码中的各个字节除了前缀码所占用之外的位。

由于ASCII字符的UTF-8编码使用单字节，而且和ASCII编码一模一样，这样所有原先使用ASCII编码的文档就可以直接解码了，无需进行任何转换，实现了完全兼容。考虑到计算机世界中英文文档的数量之多，这一点意义重大。

而对于其他非ASCII字符，则使用2~4个字节的编码来表示。其中，首字节中前置的1的个数代表该字符编码的字节数(110代表两个字节、1110代表三个字节，以此类推)，非首字节之外的剩余字节的高2位始终是10，这样就不会与ASCII字符编码以及非ASCII字符的首字节编码相冲突。

例如，假设某个字符的首字节是1110yyyy，前置有三个1，说明该字符编码总共有三个字节，必须和后面两个以10开头的字节结合才能正确解码该字符。










































